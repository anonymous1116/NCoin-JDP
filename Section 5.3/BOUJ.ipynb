{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8983e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('../../'))\n",
    "from DNN_module import Net\n",
    "\n",
    "# CPU\n",
    "torch.set_default_device(\"cpu\")\n",
    "\n",
    "%run ../../NCoinDP_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "258dd805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "# Number of data points\n",
    "#S = 500\n",
    "\n",
    "# time inteval\n",
    "\n",
    "delta = 1/12\n",
    "\n",
    "#n = int(S/delta)\n",
    "n = 1000\n",
    "\n",
    "# Observation time\n",
    "obtime = np.arange(0,n+1)/n * n * delta\n",
    "print(len(obtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4cd0c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2+cu121\n",
      "True\n",
      "NVIDIA A100-PCIE-40GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "if torch.cuda.is_available(): \n",
    " dev = \"cuda:0\" \n",
    "else: \n",
    " dev = \"cpu\"\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a2df28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_11_range = [0.1, 3]\n",
    "kappa_21_range = [0.1, 3]\n",
    "kappa_22_range = [0.1, 3]\n",
    "\n",
    "beta_1_range = [-1, 1]\n",
    "beta_2_range = [-1, 1]\n",
    "\n",
    "sigma2_1_range = [0.1, 0.5]\n",
    "sigma2_2_range = [0.1, 0.5]\n",
    "\n",
    "lambda_1_range = [0.01, 1]\n",
    "lambda_2_range = [0.01, 1]\n",
    "\n",
    "mu_1_range = [0.1,1.5]\n",
    "mu_2_range = [0.1,1.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75c9021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200000, 130]) torch.Size([40000, 130]) torch.Size([10000, 130])\n",
      "train_loss: 10.356894  [   64/200000]\n",
      "train_loss: 2.731370  [ 6464/200000]\n",
      "train_loss: 1.598500  [12864/200000]\n",
      "train_loss: 1.210528  [19264/200000]\n",
      "train_loss: 1.015450  [25664/200000]\n",
      "train_loss: 0.983386  [32064/200000]\n",
      "train_loss: 0.813227  [38464/200000]\n",
      "train_loss: 0.891353  [44864/200000]\n",
      "train_loss: 0.816257  [51264/200000]\n",
      "train_loss: 0.820574  [57664/200000]\n",
      "train_loss: 0.715423  [64064/200000]\n",
      "train_loss: 0.713527  [70464/200000]\n",
      "train_loss: 0.731132  [76864/200000]\n",
      "train_loss: 0.729888  [83264/200000]\n",
      "train_loss: 0.713949  [89664/200000]\n",
      "train_loss: 0.744366  [96064/200000]\n",
      "train_loss: 0.636937  [102464/200000]\n",
      "train_loss: 0.691487  [108864/200000]\n",
      "train_loss: 0.709109  [115264/200000]\n",
      "train_loss: 0.631166  [121664/200000]\n",
      "train_loss: 0.649398  [128064/200000]\n",
      "train_loss: 0.672767  [134464/200000]\n",
      "train_loss: 0.612035  [140864/200000]\n",
      "train_loss: 0.651607  [147264/200000]\n",
      "train_loss: 0.634693  [153664/200000]\n",
      "train_loss: 0.636934  [160064/200000]\n",
      "train_loss: 0.630605  [166464/200000]\n",
      "train_loss: 0.652496  [172864/200000]\n",
      "train_loss: 0.646142  [179264/200000]\n",
      "train_loss: 0.603835  [185664/200000]\n",
      "train_loss: 0.606161  [192064/200000]\n",
      "train_loss: 0.606773  [198464/200000]\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "train_loss 0.564841 val_loss 0.566980\n",
      "learning rate:  1e-05 sim:  0\n",
      "tensor(0.5670)\n",
      "tensor(0.4990)\n",
      "tensor(0.4584)\n",
      "tensor(0.4380)\n",
      "tensor(0.4237)\n",
      "tensor(0.4068)\n",
      "tensor(0.4001)\n",
      "tensor(0.3906)\n",
      "tensor(0.3802)\n",
      "tensor(0.3736)\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "train_loss 0.351988 val_loss 0.366178\n",
      "learning rate:  1e-05 sim:  0\n",
      "tensor(0.3662)\n",
      "tensor(0.3616)\n",
      "tensor(0.3556)\n",
      "tensor(0.3477)\n",
      "tensor(0.3450)\n",
      "tensor(0.3377)\n",
      "tensor(0.3350)\n",
      "tensor(0.3315)\n",
      "tensor(0.3245)\n",
      "tensor(0.3245)\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "train_loss 0.295259 val_loss 0.317354\n",
      "learning rate:  1e-05 sim:  0\n",
      "tensor(0.3174)\n",
      "tensor(0.3150)\n",
      "tensor(0.3150)\n",
      "tensor(0.3074)\n",
      "tensor(0.3059)\n",
      "tensor(0.3014)\n",
      "tensor(0.3014)\n",
      "tensor(0.2969)\n",
      "tensor(0.2953)\n",
      "tensor(0.2912)\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "train_loss 0.262737 val_loss 0.292010\n",
      "learning rate:  1e-05 sim:  0\n",
      "tensor(0.2912)\n",
      "tensor(0.2896)\n",
      "tensor(0.2861)\n",
      "tensor(0.2857)\n",
      "tensor(0.2843)\n",
      "tensor(0.2842)\n",
      "tensor(0.2823)\n",
      "tensor(0.2805)\n",
      "tensor(0.2786)\n",
      "tensor(0.2786)\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "train_loss 0.246525 val_loss 0.283374\n",
      "learning rate:  1e-05 sim:  0\n",
      "tensor(0.2786)\n",
      "tensor(0.2761)\n",
      "tensor(0.2760)\n",
      "tensor(0.2736)\n",
      "tensor(0.2715)\n",
      "tensor(0.2715)\n",
      "tensor(0.2715)\n",
      "tensor(0.2697)\n",
      "tensor(0.2697)\n",
      "tensor(0.2679)\n",
      "train_loss: 0.217850  [   64/200000]\n",
      "train_loss: 0.262067  [ 6464/200000]\n",
      "train_loss: 0.231991  [12864/200000]\n",
      "train_loss: 0.216846  [19264/200000]\n",
      "train_loss: 0.228038  [25664/200000]\n",
      "train_loss: 0.231301  [32064/200000]\n",
      "train_loss: 0.247030  [38464/200000]\n",
      "train_loss: 0.236225  [44864/200000]\n",
      "train_loss: 0.253455  [51264/200000]\n",
      "train_loss: 0.259376  [57664/200000]\n",
      "train_loss: 0.214320  [64064/200000]\n",
      "train_loss: 0.224242  [70464/200000]\n",
      "train_loss: 0.255487  [76864/200000]\n",
      "train_loss: 0.228696  [83264/200000]\n",
      "train_loss: 0.236627  [89664/200000]\n",
      "train_loss: 0.218976  [96064/200000]\n",
      "train_loss: 0.204769  [102464/200000]\n",
      "train_loss: 0.209581  [108864/200000]\n",
      "train_loss: 0.240303  [115264/200000]\n",
      "train_loss: 0.224255  [121664/200000]\n",
      "train_loss: 0.225516  [128064/200000]\n",
      "train_loss: 0.231964  [134464/200000]\n",
      "train_loss: 0.221087  [140864/200000]\n",
      "train_loss: 0.203108  [147264/200000]\n",
      "train_loss: 0.242736  [153664/200000]\n",
      "train_loss: 0.211653  [160064/200000]\n",
      "train_loss: 0.238761  [166464/200000]\n",
      "train_loss: 0.207901  [172864/200000]\n",
      "train_loss: 0.268805  [179264/200000]\n",
      "train_loss: 0.206248  [185664/200000]\n",
      "train_loss: 0.235038  [192064/200000]\n",
      "train_loss: 0.241415  [198464/200000]\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "train_loss 0.222700 val_loss 0.267510\n",
      "learning rate:  1e-05 sim:  0\n",
      "tensor(0.2675)\n",
      "tensor(0.2674)\n",
      "tensor(0.2665)\n",
      "tensor(0.2639)\n",
      "tensor(0.2639)\n",
      "tensor(0.2639)\n",
      "tensor(0.2639)\n",
      "tensor(0.2639)\n",
      "tensor(0.2619)\n",
      "tensor(0.2619)\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "train_loss 0.209464 val_loss 0.262091\n",
      "learning rate:  1e-05 sim:  0\n",
      "tensor(0.2619)\n",
      "tensor(0.2598)\n",
      "tensor(0.2598)\n",
      "tensor(0.2598)\n",
      "tensor(0.2591)\n",
      "tensor(0.2591)\n",
      "tensor(0.2588)\n",
      "tensor(0.2588)\n",
      "tensor(0.2588)\n",
      "tensor(0.2588)\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "train_loss 0.200422 val_loss 0.260740\n",
      "learning rate:  1e-05 sim:  0\n",
      "tensor(0.2588)\n",
      "tensor(0.2569)\n",
      "tensor(0.2569)\n",
      "tensor(0.2569)\n",
      "tensor(0.2553)\n",
      "tensor(0.2553)\n",
      "tensor(0.2553)\n",
      "tensor(0.2553)\n",
      "tensor(0.2551)\n",
      "tensor(0.2551)\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "train_loss 0.184771 val_loss 0.252502\n",
      "learning rate:  1e-05 sim:  0\n",
      "tensor(0.2525)\n",
      "tensor(0.2523)\n",
      "tensor(0.2523)\n",
      "tensor(0.2522)\n",
      "tensor(0.2522)\n",
      "tensor(0.2522)\n",
      "tensor(0.2522)\n",
      "tensor(0.2521)\n",
      "tensor(0.2521)\n",
      "tensor(0.2502)\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "train_loss 0.176026 val_loss 0.252225\n",
      "learning rate:  1e-05 sim:  0\n",
      "tensor(0.2502)\n",
      "tensor(0.2502)\n",
      "tensor(0.2502)\n",
      "tensor(0.2502)\n",
      "tensor(0.2502)\n",
      "tensor(0.2502)\n",
      "tensor(0.2502)\n",
      "tensor(0.2502)\n",
      "tensor(0.2502)\n",
      "tensor(0.2502)\n",
      "train_loss: 0.174753  [   64/200000]\n",
      "train_loss: 0.162252  [ 6464/200000]\n",
      "train_loss: 0.211383  [12864/200000]\n",
      "train_loss: 0.181240  [19264/200000]\n",
      "train_loss: 0.197503  [25664/200000]\n",
      "train_loss: 0.174360  [32064/200000]\n",
      "train_loss: 0.177119  [38464/200000]\n",
      "train_loss: 0.183179  [44864/200000]\n",
      "train_loss: 0.185070  [51264/200000]\n",
      "train_loss: 0.181598  [57664/200000]\n",
      "train_loss: 0.169335  [64064/200000]\n",
      "train_loss: 0.194530  [70464/200000]\n",
      "train_loss: 0.160274  [76864/200000]\n",
      "train_loss: 0.185596  [83264/200000]\n",
      "train_loss: 0.165039  [89664/200000]\n",
      "train_loss: 0.189334  [96064/200000]\n",
      "train_loss: 0.175631  [102464/200000]\n",
      "train_loss: 0.154290  [108864/200000]\n",
      "train_loss: 0.185881  [115264/200000]\n",
      "train_loss: 0.176494  [121664/200000]\n",
      "train_loss: 0.173576  [128064/200000]\n",
      "train_loss: 0.181502  [134464/200000]\n",
      "train_loss: 0.188150  [140864/200000]\n",
      "train_loss: 0.180217  [147264/200000]\n",
      "train_loss: 0.196764  [153664/200000]\n",
      "train_loss: 0.192245  [160064/200000]\n",
      "train_loss: 0.179573  [166464/200000]\n",
      "train_loss: 0.197169  [172864/200000]\n",
      "train_loss: 0.192286  [179264/200000]\n",
      "train_loss: 0.168832  [185664/200000]\n",
      "train_loss: 0.152866  [192064/200000]\n",
      "train_loss: 0.181985  [198464/200000]\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "train_loss 0.175547 val_loss 0.251673\n",
      "learning rate:  1.0000000000000002e-06 sim:  0\n",
      "tensor(0.2502)\n",
      "tensor(0.2502)\n",
      "tensor(0.2502)\n",
      "tensor(0.2502)\n",
      "tensor(0.2502)\n",
      "tensor(0.2502)\n",
      "tensor(0.2502)\n",
      "tensor(0.2502)\n",
      "tensor(0.2502)\n",
      "tensor(0.2502)\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "train_loss 0.168793 val_loss 0.252275\n",
      "learning rate:  1.0000000000000002e-06 sim:  0\n",
      "tensor(0.2502)\n",
      "tensor(0.2502)\n",
      "tensor(0.2502)\n",
      "tensor(0.2502)\n",
      "tensor(0.2502)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "train_loss 0.158966 val_loss 0.250182\n",
      "learning rate:  1.0000000000000002e-06 sim:  0\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "train_loss 0.158594 val_loss 0.257078\n",
      "learning rate:  1.0000000000000002e-06 sim:  0\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "train_loss 0.148216 val_loss 0.254361\n",
      "learning rate:  1.0000000000000002e-06 sim:  0\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "train_loss: 0.134822  [   64/200000]\n",
      "train_loss: 0.130605  [ 6464/200000]\n",
      "train_loss: 0.124256  [12864/200000]\n",
      "train_loss: 0.140541  [19264/200000]\n",
      "train_loss: 0.156475  [25664/200000]\n",
      "train_loss: 0.127571  [32064/200000]\n",
      "train_loss: 0.148633  [38464/200000]\n",
      "train_loss: 0.151457  [44864/200000]\n",
      "train_loss: 0.121094  [51264/200000]\n",
      "train_loss: 0.149857  [57664/200000]\n",
      "train_loss: 0.129058  [64064/200000]\n",
      "train_loss: 0.136473  [70464/200000]\n",
      "train_loss: 0.138642  [76864/200000]\n",
      "train_loss: 0.149658  [83264/200000]\n",
      "train_loss: 0.158945  [89664/200000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.172270  [96064/200000]\n",
      "train_loss: 0.152250  [102464/200000]\n",
      "train_loss: 0.133217  [108864/200000]\n",
      "train_loss: 0.130109  [115264/200000]\n",
      "train_loss: 0.139241  [121664/200000]\n",
      "train_loss: 0.137484  [128064/200000]\n",
      "train_loss: 0.154894  [134464/200000]\n",
      "train_loss: 0.140859  [140864/200000]\n",
      "train_loss: 0.144367  [147264/200000]\n",
      "train_loss: 0.143753  [153664/200000]\n",
      "train_loss: 0.155136  [160064/200000]\n",
      "train_loss: 0.155639  [166464/200000]\n",
      "train_loss: 0.127960  [172864/200000]\n",
      "train_loss: 0.144438  [179264/200000]\n",
      "train_loss: 0.159557  [185664/200000]\n",
      "train_loss: 0.148652  [192064/200000]\n",
      "train_loss: 0.160189  [198464/200000]\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "train_loss 0.143226 val_loss 0.256504\n",
      "learning rate:  1.0000000000000002e-06 sim:  0\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "train_loss 0.135466 val_loss 0.255828\n",
      "learning rate:  1.0000000000000002e-06 sim:  0\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "train_loss 0.131945 val_loss 0.259093\n",
      "learning rate:  1.0000000000000002e-06 sim:  0\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "train_loss 0.126035 val_loss 0.259810\n",
      "learning rate:  1.0000000000000002e-06 sim:  0\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "train_loss 0.121911 val_loss 0.263027\n",
      "learning rate:  1.0000000000000002e-06 sim:  0\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "train_loss: 0.129503  [   64/200000]\n",
      "train_loss: 0.151463  [ 6464/200000]\n",
      "train_loss: 0.176427  [12864/200000]\n",
      "train_loss: 0.176056  [19264/200000]\n",
      "train_loss: 0.173999  [25664/200000]\n",
      "train_loss: 0.170347  [32064/200000]\n",
      "train_loss: 0.166834  [38464/200000]\n",
      "train_loss: 0.145292  [44864/200000]\n",
      "train_loss: 0.161876  [51264/200000]\n",
      "train_loss: 0.170381  [57664/200000]\n",
      "train_loss: 0.146416  [64064/200000]\n",
      "train_loss: 0.172822  [70464/200000]\n",
      "train_loss: 0.154852  [76864/200000]\n",
      "train_loss: 0.172184  [83264/200000]\n",
      "train_loss: 0.158733  [89664/200000]\n",
      "train_loss: 0.157033  [96064/200000]\n",
      "train_loss: 0.176055  [102464/200000]\n",
      "train_loss: 0.164444  [108864/200000]\n",
      "train_loss: 0.185081  [115264/200000]\n",
      "train_loss: 0.186463  [121664/200000]\n",
      "train_loss: 0.167175  [128064/200000]\n",
      "train_loss: 0.159864  [134464/200000]\n",
      "train_loss: 0.177266  [140864/200000]\n",
      "train_loss: 0.172083  [147264/200000]\n",
      "train_loss: 0.158665  [153664/200000]\n",
      "train_loss: 0.211255  [160064/200000]\n",
      "train_loss: 0.160408  [166464/200000]\n",
      "train_loss: 0.172936  [172864/200000]\n",
      "train_loss: 0.169058  [179264/200000]\n",
      "train_loss: 0.172850  [185664/200000]\n",
      "train_loss: 0.178828  [192064/200000]\n",
      "train_loss: 0.167947  [198464/200000]\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "train_loss 0.164609 val_loss 0.253169\n",
      "learning rate:  1.0000000000000002e-07 sim:  0\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2498)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "train_loss 0.154336 val_loss 0.249877\n",
      "learning rate:  1.0000000000000002e-07 sim:  0\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "train_loss 0.147852 val_loss 0.251081\n",
      "learning rate:  1.0000000000000002e-07 sim:  0\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "train_loss 0.143200 val_loss 0.253971\n",
      "learning rate:  1.0000000000000002e-07 sim:  0\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "train_loss 0.141154 val_loss 0.258910\n",
      "learning rate:  1.0000000000000002e-07 sim:  0\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "train_loss: 0.117498  [   64/200000]\n",
      "train_loss: 0.127006  [ 6464/200000]\n",
      "train_loss: 0.141214  [12864/200000]\n",
      "train_loss: 0.139818  [19264/200000]\n",
      "train_loss: 0.150549  [25664/200000]\n",
      "train_loss: 0.136250  [32064/200000]\n",
      "train_loss: 0.128009  [38464/200000]\n",
      "train_loss: 0.115225  [44864/200000]\n",
      "train_loss: 0.140871  [51264/200000]\n",
      "train_loss: 0.136270  [57664/200000]\n",
      "train_loss: 0.129748  [64064/200000]\n",
      "train_loss: 0.123026  [70464/200000]\n",
      "train_loss: 0.140225  [76864/200000]\n",
      "train_loss: 0.124097  [83264/200000]\n",
      "train_loss: 0.140148  [89664/200000]\n",
      "train_loss: 0.142672  [96064/200000]\n",
      "train_loss: 0.141097  [102464/200000]\n",
      "train_loss: 0.144142  [108864/200000]\n",
      "train_loss: 0.125418  [115264/200000]\n",
      "train_loss: 0.163831  [121664/200000]\n",
      "train_loss: 0.152372  [128064/200000]\n",
      "train_loss: 0.131126  [134464/200000]\n",
      "train_loss: 0.154774  [140864/200000]\n",
      "train_loss: 0.138827  [147264/200000]\n",
      "train_loss: 0.144182  [153664/200000]\n",
      "train_loss: 0.137787  [160064/200000]\n",
      "train_loss: 0.140017  [166464/200000]\n",
      "train_loss: 0.135472  [172864/200000]\n",
      "train_loss: 0.161671  [179264/200000]\n",
      "train_loss: 0.130186  [185664/200000]\n",
      "train_loss: 0.121404  [192064/200000]\n",
      "train_loss: 0.146250  [198464/200000]\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "train_loss 0.139366 val_loss 0.264051\n",
      "learning rate:  1.0000000000000002e-07 sim:  0\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "train_loss 0.126375 val_loss 0.257574\n",
      "learning rate:  1.0000000000000002e-07 sim:  0\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "train_loss 0.122642 val_loss 0.261113\n",
      "learning rate:  1.0000000000000002e-07 sim:  0\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "train_loss 0.116071 val_loss 0.261009\n",
      "learning rate:  1.0000000000000002e-07 sim:  0\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "train_loss 0.114943 val_loss 0.265477\n",
      "learning rate:  1.0000000000000002e-07 sim:  0\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "train_loss: 0.142687  [   64/200000]\n",
      "train_loss: 0.166842  [ 6464/200000]\n",
      "train_loss: 0.162698  [12864/200000]\n",
      "train_loss: 0.159252  [19264/200000]\n",
      "train_loss: 0.173343  [25664/200000]\n",
      "train_loss: 0.187684  [32064/200000]\n",
      "train_loss: 0.158079  [38464/200000]\n",
      "train_loss: 0.171304  [44864/200000]\n",
      "train_loss: 0.159437  [51264/200000]\n",
      "train_loss: 0.159046  [57664/200000]\n",
      "train_loss: 0.146483  [64064/200000]\n",
      "train_loss: 0.182814  [70464/200000]\n",
      "train_loss: 0.168867  [76864/200000]\n",
      "train_loss: 0.150221  [83264/200000]\n",
      "train_loss: 0.196856  [89664/200000]\n",
      "train_loss: 0.167777  [96064/200000]\n",
      "train_loss: 0.168012  [102464/200000]\n",
      "train_loss: 0.153376  [108864/200000]\n",
      "train_loss: 0.167756  [115264/200000]\n",
      "train_loss: 0.156406  [121664/200000]\n",
      "train_loss: 0.160409  [128064/200000]\n",
      "train_loss: 0.158857  [134464/200000]\n",
      "train_loss: 0.186269  [140864/200000]\n",
      "train_loss: 0.158129  [147264/200000]\n",
      "train_loss: 0.197660  [153664/200000]\n",
      "train_loss: 0.153363  [160064/200000]\n",
      "train_loss: 0.173911  [166464/200000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.160486  [172864/200000]\n",
      "train_loss: 0.150348  [179264/200000]\n",
      "train_loss: 0.162356  [185664/200000]\n",
      "train_loss: 0.172451  [192064/200000]\n",
      "train_loss: 0.154986  [198464/200000]\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "train_loss 0.164454 val_loss 0.255418\n",
      "learning rate:  1.0000000000000004e-08 sim:  0\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "train_loss 0.156442 val_loss 0.255619\n",
      "learning rate:  1.0000000000000004e-08 sim:  0\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "train_loss 0.149404 val_loss 0.255134\n",
      "learning rate:  1.0000000000000004e-08 sim:  0\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "train_loss 0.139859 val_loss 0.252900\n",
      "learning rate:  1.0000000000000004e-08 sim:  0\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "train_loss 0.139139 val_loss 0.258838\n",
      "learning rate:  1.0000000000000004e-08 sim:  0\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "tensor(0.2495)\n",
      "torch.Size([200000, 130]) torch.Size([40000, 130]) torch.Size([10000, 130])\n",
      "train_loss: 9.852623  [   64/200000]\n",
      "train_loss: 2.686011  [ 6464/200000]\n",
      "train_loss: 1.352558  [12864/200000]\n",
      "train_loss: 1.224620  [19264/200000]\n",
      "train_loss: 0.985933  [25664/200000]\n",
      "train_loss: 0.965821  [32064/200000]\n",
      "train_loss: 0.879056  [38464/200000]\n",
      "train_loss: 0.831877  [44864/200000]\n",
      "train_loss: 0.877534  [51264/200000]\n",
      "train_loss: 0.747639  [57664/200000]\n",
      "train_loss: 0.801441  [64064/200000]\n",
      "train_loss: 0.733533  [70464/200000]\n",
      "train_loss: 0.731678  [76864/200000]\n",
      "train_loss: 0.744252  [83264/200000]\n",
      "train_loss: 0.765270  [89664/200000]\n",
      "train_loss: 0.762790  [96064/200000]\n",
      "train_loss: 0.717634  [102464/200000]\n",
      "train_loss: 0.646682  [108864/200000]\n",
      "train_loss: 0.694718  [115264/200000]\n",
      "train_loss: 0.774808  [121664/200000]\n",
      "train_loss: 0.714806  [128064/200000]\n",
      "train_loss: 0.617909  [134464/200000]\n",
      "train_loss: 0.618358  [140864/200000]\n",
      "train_loss: 0.673814  [147264/200000]\n",
      "train_loss: 0.614343  [153664/200000]\n",
      "train_loss: 0.650453  [160064/200000]\n",
      "train_loss: 0.608348  [166464/200000]\n",
      "train_loss: 0.674857  [172864/200000]\n",
      "train_loss: 0.569390  [179264/200000]\n",
      "train_loss: 0.600410  [185664/200000]\n",
      "train_loss: 0.602485  [192064/200000]\n",
      "train_loss: 0.679276  [198464/200000]\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "train_loss 0.568081 val_loss 0.575570\n",
      "learning rate:  1e-05 sim:  1\n",
      "tensor(0.5756)\n",
      "tensor(0.5123)\n",
      "tensor(0.5123)\n",
      "tensor(0.4884)\n",
      "tensor(0.4576)\n",
      "tensor(0.4149)\n",
      "tensor(0.4010)\n",
      "tensor(0.3994)\n",
      "tensor(0.3863)\n",
      "tensor(0.3753)\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "train_loss 0.360223 val_loss 0.377440\n",
      "learning rate:  1e-05 sim:  1\n",
      "tensor(0.3753)\n",
      "tensor(0.3668)\n",
      "tensor(0.3619)\n",
      "tensor(0.3561)\n",
      "tensor(0.3491)\n",
      "tensor(0.3455)\n",
      "tensor(0.3428)\n",
      "tensor(0.3379)\n",
      "tensor(0.3334)\n",
      "tensor(0.3305)\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "train_loss 0.309867 val_loss 0.333846\n",
      "learning rate:  1e-05 sim:  1\n",
      "tensor(0.3305)\n",
      "tensor(0.3236)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from DNN_module import Net\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Default : cuda\n",
    "torch.set_default_device('cuda')\n",
    "\n",
    "# Define the batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Number of Epochs\n",
    "N_EPOCHS = 350\n",
    "    \n",
    "# Number of Simulations\n",
    "sims = 10\n",
    "\n",
    "# Number of observation\n",
    "#nums = [500, 1000, 3000]\n",
    "\n",
    "for sim in range(0, 4):\n",
    "        # Data import\n",
    "        sim_path = \"../../../depot_hyun/hyun/BOUJ/BOUJ_sim\" + str(sim) + \"_\" + str(n)+ \".pt\"\n",
    "        [X_raw, output] = torch.load(sim_path)\n",
    "        X_raw = X_raw.to(\"cpu\")\n",
    "        \n",
    "        \n",
    "        c_tmp = torch.quantile(X_raw, torch.tensor([.0001,.9999], device = \"cpu\"), 0)\n",
    "        a = torch.reshape(c_tmp[0], (1, c_tmp.size()[1]))\n",
    "        b = torch.reshape(c_tmp[1], (1, c_tmp.size()[1]))\n",
    "\n",
    "        X = torch.clone((X_raw - a) / (b - a))\n",
    "        \n",
    "        scale_path = \"BOUJ_sim/BOUJ_scale/BOUJ_scale\" + str(sim) + \"_\" + str(n)+ \".pt\"\n",
    "        torch.save([a,b], scale_path)\n",
    "\n",
    "        \n",
    "        torch.set_default_device(dev)\n",
    "        X = X.to(dev)\n",
    "        output = output.to(dev)\n",
    "        \n",
    "        L = X.size()[0]\n",
    "        Lval = 40000\n",
    "        Ltest = 10000\n",
    "        \n",
    "\n",
    "        X_train = X[range(0,L-Lval-Ltest),:]\n",
    "        y_train = output[range(0,L-Lval-Ltest),:]\n",
    "\n",
    "        X_val = X[range(L-Lval-Ltest,L-Ltest),:]\n",
    "        y_val = output[range(L-Lval-Ltest,L-Ltest),:]\n",
    "\n",
    "        X_test = X[range(L-Ltest,L),:]\n",
    "        y_test = output[range(L-Ltest,L),:]\n",
    "\n",
    "        print(X_train.size(), X_val.size(),X_test.size())\n",
    "        \n",
    "        \n",
    "        # Use torch.utils.data to create a DataLoader that will take care of creating batches\n",
    "        dataset = TensorDataset(X_train, y_train)\n",
    "        dataloader = DataLoader(dataset, batch_size = BATCH_SIZE, shuffle=True, generator=torch.Generator(device=device))\n",
    "\n",
    "        # Get the dataset size for printing (it is equal to N_SAMPLES)\n",
    "        dataset_size = len(dataloader.dataset)\n",
    "        \n",
    "        # Define the input and output dimensions\n",
    "        D_in, H, H2, H3, D_out = X_train.size()[1], 1024, 1024, 1024, y_train.size()[1]\n",
    "\n",
    "        # Create an instance of the Net class with specified dimensions\n",
    "        torch.manual_seed(2725)\n",
    "        net = Net(D_in, D_out, H = H, H2 = H2, H3 = H3)\n",
    "\n",
    "        # Model name\n",
    "        model_save_name = 'BOUJ_nets/BOUJ_' + str(sim) + '_'  + str(n) + '.pt'\n",
    "        path = F\"./{model_save_name}\"\n",
    "\n",
    "        # The nn package also contains definitions of popular loss functions; in this case we will use Mean Squared Error (MSE) as our loss function.\n",
    "        def weighted_mse_loss(input, target, weight):\n",
    "            return (weight * (input - target) ** 2).sum()\n",
    "    \n",
    "        out_range = [torch.quantile(y_train,.01, 0).detach().cpu().numpy(), \n",
    "                     torch.quantile(y_train,.99, 0).detach().cpu().numpy()]\n",
    "        weight_1 = torch.tensor(1/(out_range[1] - out_range[0])**2)\n",
    "\n",
    "        learning_rate = 1e-5\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "        train_error_plt = []\n",
    "        val_error_plt = []\n",
    "\n",
    "        torch.manual_seed(2000 + sim)\n",
    "        # Loop over epochs\n",
    "        file = open(\"log/BOUJ_sim.txt\",\"a\")\n",
    "        file.write(sim_path + \"start\" + \"sim: \" + str(sim)+ \"\\n\")\n",
    "        file.close()\n",
    "        \n",
    "        # Loop over epochs\n",
    "        for epoch in range(N_EPOCHS):\n",
    "            for id_batch, (x_batch, y_batch) in enumerate(dataloader):\n",
    "                y_batch_pred = net(x_batch)\n",
    "                loss = weighted_mse_loss(y_batch, y_batch_pred, weight_1)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if epoch % 50 ==0 and id_batch % 100 == 0:\n",
    "                    loss, current = loss.item(), (id_batch + 1)* len(x_batch)\n",
    "                    print(f\"train_loss: {loss/BATCH_SIZE:>7f}  [{current:>5d}/{dataset_size:>5d}]\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                net.eval()\n",
    "                theta_pred_train = net(X_train)\n",
    "                train_loss = weighted_mse_loss(y_train, theta_pred_train, weight_1) / (L-Lval-Ltest)\n",
    "                train_error_plt.append(train_loss.to(\"cpu\"))\n",
    "\n",
    "                theta_pred_val = net(X_val)\n",
    "                val_loss = weighted_mse_loss(y_val, theta_pred_val, weight_1) / Lval\n",
    "                val_error_plt.append(val_loss.to(\"cpu\"))\n",
    "\n",
    "            if epoch % 10 ==0:\n",
    "                print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "                print(f\"train_loss {train_loss:>7f} val_loss {val_loss:>7f}\")\n",
    "                print(\"learning rate: \", learning_rate, \"sim: \", sim)\n",
    "\n",
    "            # Choose Best Model\n",
    "            if val_error_plt[epoch] == np.min(val_error_plt):\n",
    "                best=epoch\n",
    "                torch.save(net.state_dict(), path)\n",
    "\n",
    "            if epoch % 100 ==99:\n",
    "                net.load_state_dict(torch.load(path))\n",
    "                learning_rate = max(learning_rate * 1e-1, 1e-9)\n",
    "            print(min(val_error_plt))\n",
    "            \n",
    "            \n",
    "        file = open(\"log/BOUJ_sim.txt\",\"a\")\n",
    "        file.write(sim_path + \"end\" + \"sim: \" + str(sim) + \"\\n\")\n",
    "        file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a567a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(val_error_plt))\n",
    "net.load_state_dict(torch.load(path))\n",
    "net.eval()\n",
    "torch.set_default_device(\"cpu\")\n",
    "\n",
    "net = net.to(\"cpu\")\n",
    "X_test = X_test.to(\"cpu\")\n",
    "y_test = y_test.to(\"cpu\")\n",
    "\n",
    "true_name = [r'true $\\kappa_{11}$', r'true $\\kappa_{21}$', r'true $\\kappa_{22}$', \n",
    "             r'true $\\beta_1$', r'true $\\beta_2$', r'true $\\sigma_1^2$', r'true $\\sigma_2^2$', \n",
    "             r'true $\\lambda_1$', r'true $\\lambda_2$', r'true $\\mu_1$',r'true $\\mu_2$']\n",
    "#true_name = [r'true $\\kappa$', r'true $\\beta$', r'true $\\lambda$', r'true $\\mu$']\n",
    "\n",
    "esti_name = [r'$\\hat{\\kappa_{11}}$', r'$\\hat{\\kappa_{21}}$', r'$\\hat{\\kappa_{22}}$', \n",
    "             r'$\\hat{\\beta_1}$', r'$\\hat{\\beta_2}$', r'$\\hat{\\sigma_1^2}$',r'$\\hat{\\sigma_2^2}$',\n",
    "             r'$\\hat{\\lambda_1}$', r'$\\hat{\\lambda_2}$', r'$\\hat{\\mu_1}$', r'$\\hat{\\mu_2}$']\n",
    "#esti_name = [r'$\\hat{\\kappa}$', r'$\\hat{\\beta}$', r'$\\hat{\\lambda}$', r'$\\hat{\\mu}$']\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    tmp = net(X_test)\n",
    "\n",
    "## Plot for model checking\n",
    "lim = [torch.min(output,0).values.detach().cpu().numpy(), torch.max(output,0).values.detach().cpu().numpy()]\n",
    "lim2 = lim[1] - lim[0]\n",
    "lim = [lim[0] - lim2 * 1/20, lim[1] + lim * 1/20 ]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(tmp), figsize=(5 * len(tmp),5))\n",
    "fig.suptitle('Model Checking')\n",
    "\n",
    "for i in range(len(tmp)):\n",
    "    lim1 = lim[i]\n",
    "    tmp1 = tmp[:,i].detach().cpu().numpy().tolist() \n",
    "    axes[i].scatter(y_test[:,i], tmp1, marker='o', color='b', s= 1)\n",
    "    axes[i].set_xlabel(true_name[i], fontsize=15)\n",
    "    axes[i].set_ylabel(esti_name[i], fontsize=15)\n",
    "    axes[i].plot(np.linspace(lim1[0], lim1[1], 1000), np.linspace(lim1[0], lim1[1], 1000), color = \"red\", linestyle='dashed', linewidth = 2.5)\n",
    "    axes[i].set_axisbelow(True)\n",
    "    axes[i].grid(color='gray', linestyle='dashed')\n",
    "    axes[i].set_ylim(lim1)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"BOUJ_plots/BOUJ_baslines.pdf\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (My sbi_pack Kernel)",
   "language": "python",
   "name": "sbi_pack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
