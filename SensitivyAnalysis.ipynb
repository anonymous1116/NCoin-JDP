{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "237ab407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from DNN_module import Net\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10b6095a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9283\n",
      "135427\n",
      "532995\n"
     ]
    }
   ],
   "source": [
    "net0 = Net(D_in = 5, D_out = 3, H = 64, H2 = 64, H3 = 64)\n",
    "net1 = Net(D_in = 5, D_out = 3, H = 256, H2 = 256, H3 = 256)\n",
    "net2 = Net(D_in = 5, D_out = 3, H = 512, H2 = 512, H3 = 512)\n",
    "\n",
    "\n",
    "def total_params(net):\n",
    "    return sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "\n",
    "print(total_params(net0))\n",
    "print(total_params(net1))\n",
    "print(total_params(net2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c701050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run NCoinDP_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62f9a61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2+cu121\n",
      "True\n",
      "NVIDIA A10\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "if torch.cuda.is_available(): \n",
    " dev = \"cuda:0\" \n",
    "else: \n",
    " dev = \"cpu\"\n",
    "device = torch.device(dev) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ae5881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 15.286289  [   64/100000]\n",
      "train_loss: 9.811146  [ 6464/100000]\n",
      "train_loss: 9.239730  [12864/100000]\n",
      "train_loss: 6.517861  [19264/100000]\n",
      "train_loss: 4.120437  [25664/100000]\n",
      "train_loss: 3.613735  [32064/100000]\n",
      "train_loss: 2.958387  [38464/100000]\n",
      "train_loss: 2.063457  [44864/100000]\n",
      "train_loss: 1.852620  [51264/100000]\n",
      "train_loss: 1.245010  [57664/100000]\n",
      "train_loss: 0.856904  [64064/100000]\n",
      "train_loss: 0.645236  [70464/100000]\n",
      "train_loss: 0.373699  [76864/100000]\n",
      "train_loss: 0.368848  [83264/100000]\n",
      "train_loss: 0.357799  [89664/100000]\n",
      "train_loss: 0.215923  [96064/100000]\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "train_loss 0.246279 val_loss 0.249988\n",
      "learning rate:  0.0001 , num:  64 , sim:  0\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "train_loss 0.107364 val_loss 0.108244\n",
      "learning rate:  0.0001 , num:  64 , sim:  0\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "train_loss 0.105103 val_loss 0.106075\n",
      "learning rate:  0.0001 , num:  64 , sim:  0\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "train_loss 0.118463 val_loss 0.119743\n",
      "learning rate:  0.0001 , num:  64 , sim:  0\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "train_loss 0.108967 val_loss 0.110523\n",
      "learning rate:  0.0001 , num:  64 , sim:  0\n",
      "train_loss: 0.118646  [   64/100000]\n",
      "train_loss: 0.131377  [ 6464/100000]\n",
      "train_loss: 0.094242  [12864/100000]\n",
      "train_loss: 0.132071  [19264/100000]\n",
      "train_loss: 0.103081  [25664/100000]\n",
      "train_loss: 0.080683  [32064/100000]\n",
      "train_loss: 0.078134  [38464/100000]\n",
      "train_loss: 0.096260  [44864/100000]\n",
      "train_loss: 0.126298  [51264/100000]\n",
      "train_loss: 0.082730  [57664/100000]\n",
      "train_loss: 0.083144  [64064/100000]\n",
      "train_loss: 0.122278  [70464/100000]\n",
      "train_loss: 0.094787  [76864/100000]\n",
      "train_loss: 0.131054  [83264/100000]\n",
      "train_loss: 0.101835  [89664/100000]\n",
      "train_loss: 0.089846  [96064/100000]\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "train_loss 0.131990 val_loss 0.133708\n",
      "learning rate:  0.0001 , num:  64 , sim:  0\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "train_loss 0.102059 val_loss 0.102810\n",
      "learning rate:  0.0001 , num:  64 , sim:  0\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "train_loss 0.112126 val_loss 0.112348\n",
      "learning rate:  0.0001 , num:  64 , sim:  0\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "train_loss 0.103713 val_loss 0.104211\n",
      "learning rate:  0.0001 , num:  64 , sim:  0\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "train_loss 0.117015 val_loss 0.116979\n",
      "learning rate:  0.0001 , num:  64 , sim:  0\n",
      "train_loss: 0.103120  [   64/100000]\n",
      "train_loss: 0.073618  [ 6464/100000]\n",
      "train_loss: 0.111804  [12864/100000]\n",
      "train_loss: 0.120015  [19264/100000]\n",
      "train_loss: 0.090524  [25664/100000]\n",
      "train_loss: 0.115248  [32064/100000]\n",
      "train_loss: 0.115613  [38464/100000]\n",
      "train_loss: 0.128483  [44864/100000]\n",
      "train_loss: 0.086443  [51264/100000]\n",
      "train_loss: 0.106472  [57664/100000]\n",
      "train_loss: 0.090569  [64064/100000]\n",
      "train_loss: 0.106406  [70464/100000]\n",
      "train_loss: 0.093315  [76864/100000]\n",
      "train_loss: 0.099033  [83264/100000]\n",
      "train_loss: 0.104945  [89664/100000]\n",
      "train_loss: 0.108958  [96064/100000]\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "train_loss 0.100458 val_loss 0.101148\n",
      "learning rate:  1e-05 , num:  64 , sim:  0\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "train_loss 0.113825 val_loss 0.113992\n",
      "learning rate:  1e-05 , num:  64 , sim:  0\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "train_loss 0.104332 val_loss 0.104684\n",
      "learning rate:  1e-05 , num:  64 , sim:  0\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "train_loss 0.101818 val_loss 0.102503\n",
      "learning rate:  1e-05 , num:  64 , sim:  0\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "train_loss 0.105285 val_loss 0.105867\n",
      "learning rate:  1e-05 , num:  64 , sim:  0\n",
      "train_loss: 0.080377  [   64/100000]\n",
      "train_loss: 0.146183  [ 6464/100000]\n",
      "train_loss: 0.090895  [12864/100000]\n",
      "train_loss: 0.093850  [19264/100000]\n",
      "train_loss: 0.103475  [25664/100000]\n",
      "train_loss: 0.119536  [32064/100000]\n",
      "train_loss: 0.097143  [38464/100000]\n",
      "train_loss: 0.097000  [44864/100000]\n",
      "train_loss: 0.103337  [51264/100000]\n",
      "train_loss: 0.088549  [57664/100000]\n",
      "train_loss: 0.081966  [64064/100000]\n",
      "train_loss: 0.103619  [70464/100000]\n",
      "train_loss: 0.085382  [76864/100000]\n",
      "train_loss: 0.113218  [83264/100000]\n",
      "train_loss: 0.095364  [89664/100000]\n",
      "train_loss: 0.105109  [96064/100000]\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "train_loss 0.114000 val_loss 0.114366\n",
      "learning rate:  1e-05 , num:  64 , sim:  0\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "train_loss 0.103671 val_loss 0.104802\n",
      "learning rate:  1e-05 , num:  64 , sim:  0\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "train_loss 0.100933 val_loss 0.101911\n",
      "learning rate:  1e-05 , num:  64 , sim:  0\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "train_loss 0.104673 val_loss 0.105358\n",
      "learning rate:  1e-05 , num:  64 , sim:  0\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "train_loss 0.103202 val_loss 0.103769\n",
      "learning rate:  1e-05 , num:  64 , sim:  0\n",
      "train_loss: 0.093878  [   64/100000]\n",
      "train_loss: 0.118085  [ 6464/100000]\n",
      "train_loss: 0.082921  [12864/100000]\n",
      "train_loss: 0.142530  [19264/100000]\n",
      "train_loss: 0.131273  [25664/100000]\n",
      "train_loss: 0.106153  [32064/100000]\n",
      "train_loss: 0.093178  [38464/100000]\n",
      "train_loss: 0.128024  [44864/100000]\n",
      "train_loss: 0.120241  [51264/100000]\n",
      "train_loss: 0.079050  [57664/100000]\n",
      "train_loss: 0.116302  [64064/100000]\n",
      "train_loss: 0.078027  [70464/100000]\n",
      "train_loss: 0.122594  [76864/100000]\n",
      "train_loss: 0.090867  [83264/100000]\n",
      "train_loss: 0.080907  [89664/100000]\n",
      "train_loss: 0.132374  [96064/100000]\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "train_loss 0.111916 val_loss 0.112439\n",
      "learning rate:  1.0000000000000002e-06 , num:  64 , sim:  0\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "train_loss 0.116499 val_loss 0.117978\n",
      "learning rate:  1.0000000000000002e-06 , num:  64 , sim:  0\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "train_loss 0.135392 val_loss 0.135207\n",
      "learning rate:  1.0000000000000002e-06 , num:  64 , sim:  0\n"
     ]
    }
   ],
   "source": [
    "# Default : cuda\n",
    "torch.set_default_device('cuda')\n",
    "\n",
    "# Number of train and validation data\n",
    "L_train = 100000\n",
    "L_val   =  50000\n",
    "\n",
    "# Define the batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Number of Epochs\n",
    "N_EPOCHS = 250\n",
    "    \n",
    "# Number of Simulations\n",
    "sims = 50\n",
    "\n",
    "# net\n",
    "nets = [64, 256, 512]\n",
    "\n",
    "for sim in range(0, sims):\n",
    "    tmp = \"syn_data/OU_sim_n3000.pt\"\n",
    "    tmp = torch.load(tmp)\n",
    "\n",
    "    # Data import\n",
    "    [X, Y] = tmp\n",
    "    X = X.to(device)\n",
    "    Y = Y.to(device)\n",
    "\n",
    "    torch.manual_seed(1000 + sim)\n",
    "    indexes = torch.randperm(L_train + L_val)\n",
    "\n",
    "    # Divide Data\n",
    "    X_train = X[indexes[0:L_train]]\n",
    "    Y_train = Y[indexes[0:L_train]]\n",
    "\n",
    "    X_val = X[indexes[L_train:(L_train + L_val)]]\n",
    "    Y_val = Y[indexes[L_train:(L_train + L_val)]]\n",
    "    \n",
    "    # Use torch.utils.data to create a DataLoader that will take care of creating batches\n",
    "    dataset = TensorDataset(X_train, Y_train)\n",
    "    dataloader = DataLoader(dataset, batch_size = BATCH_SIZE, shuffle=True, generator=torch.Generator(device=device))\n",
    "\n",
    "    # Get the dataset size for printing (it is equal to N_SAMPLES)\n",
    "    dataset_size = len(dataloader.dataset)\n",
    "\n",
    "    # Define the input and output dimensions\n",
    "    D_in, D_out = X_train.size()[1], Y_train.size()[1]\n",
    "\n",
    "\n",
    "    for net_num in nets:\n",
    "        # Create an instance of the Net class with specified dimensions\n",
    "        net = Net(D_in = D_in, D_out = D_out, H = net_num, H2 = net_num, H3 =net_num)\n",
    "\n",
    "        # Model name\n",
    "        model_save_name = 'nets/SA/net'+str(net_num)+\"_\"+str(sim)+'.pt'\n",
    "        path = F\"./{model_save_name}\"\n",
    "\n",
    "        # The nn package also contains definitions of popular loss functions; in this case we will use Mean Squared Error (MSE) as our loss function.\n",
    "        loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "        learning_rate = 1e-4\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "        train_error_plt = []\n",
    "        val_error_plt = []\n",
    "\n",
    "        torch.manual_seed(2000 + sim)\n",
    "\n",
    "        # Loop over epochs\n",
    "        for epoch in range(N_EPOCHS):\n",
    "            for id_batch, (x_batch, y_batch) in enumerate(dataloader):\n",
    "                y_batch_pred = net(x_batch)\n",
    "                loss = loss_fn(y_batch_pred, y_batch)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if epoch % 50 ==0 and id_batch % 100 == 0:\n",
    "                    loss, current = loss.item(), (id_batch + 1)* len(x_batch)\n",
    "                    print(f\"train_loss: {loss/BATCH_SIZE:>7f}  [{current:>5d}/{dataset_size:>5d}]\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                   net.eval()\n",
    "                   theta_pred_train = net(X_train)\n",
    "                   train_loss = loss_fn(theta_pred_train,Y_train) / L_train\n",
    "                   train_error_plt = np.append(train_error_plt, train_loss.to(\"cpu\"))\n",
    "\n",
    "                   theta_pred_val = net(X_val)\n",
    "                   val_loss = loss_fn(Y_val, theta_pred_val) / L_val\n",
    "                   val_error_plt = np.append(val_error_plt, val_loss.to(\"cpu\"))\n",
    "\n",
    "            if epoch % 10 ==0:\n",
    "                print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "                print(f\"train_loss {train_loss:>7f} val_loss {val_loss:>7f}\")\n",
    "                print(\"learning rate: \", learning_rate, \", net num: \", net_num, \", sim: \", sim)\n",
    "\n",
    "            ## Choose Best Model\n",
    "            if val_error_plt[epoch] == np.min(val_error_plt):\n",
    "                 best=epoch\n",
    "                 torch.save(net.state_dict(), path)\n",
    "\n",
    "            if epoch % 100 ==99:\n",
    "                net.load_state_dict(torch.load(path))\n",
    "                learning_rate = max(learning_rate * 1e-1, 1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbe6a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad3c417",
   "metadata": {},
   "outputs": [],
   "source": [
    "#net.load_state_dict(torch.load(\"./OU_0.pt\"))\n",
    "net.load_state_dict(torch.load(path))\n",
    "\n",
    "torch.set_default_device('cpu')\n",
    "print(np.min(val_error_plt))\n",
    "print(np.argmin(val_error_plt))\n",
    "plt.plot(np.arange(N_EPOCHS), train_error_plt, color = \"r\")\n",
    "plt.plot(np.arange(N_EPOCHS), val_error_plt)\n",
    "plt.legend([\"train\", \"validation\"], loc =\"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd326ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to(torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e260a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypackages2",
   "language": "python",
   "name": "mypackages2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
