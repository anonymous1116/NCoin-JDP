{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "237ab407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from DNN_module import Net\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c701050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run NCoinDP_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62f9a61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2+cu121\n",
      "True\n",
      "NVIDIA A10\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "if torch.cuda.is_available(): \n",
    " dev = \"cuda:0\" \n",
    "else: \n",
    " dev = \"cpu\"\n",
    "device = torch.device(dev) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ae5881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 16.841385  [   64/200000]\n",
      "train_loss: 6.429161  [ 6464/200000]\n",
      "train_loss: 5.771806  [12864/200000]\n",
      "train_loss: 3.279193  [19264/200000]\n",
      "train_loss: 1.398955  [25664/200000]\n",
      "train_loss: 0.800822  [32064/200000]\n",
      "train_loss: 0.526072  [38464/200000]\n",
      "train_loss: 0.339178  [44864/200000]\n",
      "train_loss: 0.191689  [51264/200000]\n",
      "train_loss: 0.211995  [57664/200000]\n",
      "train_loss: 0.215071  [64064/200000]\n",
      "train_loss: 0.178395  [70464/200000]\n",
      "train_loss: 0.148805  [76864/200000]\n",
      "train_loss: 0.131294  [83264/200000]\n",
      "train_loss: 0.115533  [89664/200000]\n",
      "train_loss: 0.178908  [96064/200000]\n",
      "train_loss: 0.121493  [102464/200000]\n",
      "train_loss: 0.167306  [108864/200000]\n",
      "train_loss: 0.169280  [115264/200000]\n",
      "train_loss: 0.187463  [121664/200000]\n",
      "train_loss: 0.214297  [128064/200000]\n",
      "train_loss: 0.149557  [134464/200000]\n",
      "train_loss: 0.335281  [140864/200000]\n",
      "train_loss: 0.242648  [147264/200000]\n",
      "train_loss: 0.122909  [153664/200000]\n",
      "train_loss: 0.150471  [160064/200000]\n",
      "train_loss: 0.178709  [166464/200000]\n",
      "train_loss: 0.110915  [172864/200000]\n",
      "train_loss: 0.143883  [179264/200000]\n",
      "train_loss: 0.170127  [185664/200000]\n",
      "train_loss: 0.126409  [192064/200000]\n",
      "train_loss: 0.136596  [198464/200000]\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "train_loss 0.271244 val_loss 0.270809\n",
      "learning rate:  0.0001 , L:  200000 , sim:  0\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "train_loss 0.106175 val_loss 0.106002\n",
      "learning rate:  0.0001 , L:  200000 , sim:  0\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "train_loss 0.109585 val_loss 0.108680\n",
      "learning rate:  0.0001 , L:  200000 , sim:  0\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "train_loss 0.126426 val_loss 0.126555\n",
      "learning rate:  0.0001 , L:  200000 , sim:  0\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "train_loss 0.119175 val_loss 0.118613\n",
      "learning rate:  0.0001 , L:  200000 , sim:  0\n",
      "train_loss: 0.103248  [   64/200000]\n",
      "train_loss: 0.157417  [ 6464/200000]\n",
      "train_loss: 0.077890  [12864/200000]\n",
      "train_loss: 0.120651  [19264/200000]\n",
      "train_loss: 0.141389  [25664/200000]\n",
      "train_loss: 0.099653  [32064/200000]\n",
      "train_loss: 0.112300  [38464/200000]\n",
      "train_loss: 0.153031  [44864/200000]\n",
      "train_loss: 0.187629  [51264/200000]\n",
      "train_loss: 0.087504  [57664/200000]\n",
      "train_loss: 0.115109  [64064/200000]\n",
      "train_loss: 0.107606  [70464/200000]\n",
      "train_loss: 0.131098  [76864/200000]\n",
      "train_loss: 0.073958  [83264/200000]\n",
      "train_loss: 0.101372  [89664/200000]\n",
      "train_loss: 0.103490  [96064/200000]\n",
      "train_loss: 0.100078  [102464/200000]\n",
      "train_loss: 0.135374  [108864/200000]\n",
      "train_loss: 0.128122  [115264/200000]\n",
      "train_loss: 0.070873  [121664/200000]\n",
      "train_loss: 0.104846  [128064/200000]\n",
      "train_loss: 0.090347  [134464/200000]\n",
      "train_loss: 0.117932  [140864/200000]\n",
      "train_loss: 0.135215  [147264/200000]\n",
      "train_loss: 0.109221  [153664/200000]\n",
      "train_loss: 0.092579  [160064/200000]\n",
      "train_loss: 0.172442  [166464/200000]\n",
      "train_loss: 0.173813  [172864/200000]\n",
      "train_loss: 0.139875  [179264/200000]\n",
      "train_loss: 0.147703  [185664/200000]\n",
      "train_loss: 0.097907  [192064/200000]\n",
      "train_loss: 0.104971  [198464/200000]\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "train_loss 0.105677 val_loss 0.104906\n",
      "learning rate:  0.0001 , L:  200000 , sim:  0\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "train_loss 0.103602 val_loss 0.103198\n",
      "learning rate:  0.0001 , L:  200000 , sim:  0\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "train_loss 0.103253 val_loss 0.102785\n",
      "learning rate:  0.0001 , L:  200000 , sim:  0\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "train_loss 0.125382 val_loss 0.124448\n",
      "learning rate:  0.0001 , L:  200000 , sim:  0\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "train_loss 0.104316 val_loss 0.104279\n",
      "learning rate:  0.0001 , L:  200000 , sim:  0\n",
      "train_loss: 0.076851  [   64/200000]\n",
      "train_loss: 0.122088  [ 6464/200000]\n",
      "train_loss: 0.112312  [12864/200000]\n",
      "train_loss: 0.092972  [19264/200000]\n",
      "train_loss: 0.149850  [25664/200000]\n",
      "train_loss: 0.138976  [32064/200000]\n",
      "train_loss: 0.111654  [38464/200000]\n",
      "train_loss: 0.161128  [44864/200000]\n",
      "train_loss: 0.133499  [51264/200000]\n",
      "train_loss: 0.118582  [57664/200000]\n",
      "train_loss: 0.115504  [64064/200000]\n",
      "train_loss: 0.091595  [70464/200000]\n",
      "train_loss: 0.082296  [76864/200000]\n",
      "train_loss: 0.123884  [83264/200000]\n",
      "train_loss: 0.079717  [89664/200000]\n",
      "train_loss: 0.177099  [96064/200000]\n",
      "train_loss: 0.108619  [102464/200000]\n",
      "train_loss: 0.082795  [108864/200000]\n",
      "train_loss: 0.084541  [115264/200000]\n",
      "train_loss: 0.136834  [121664/200000]\n",
      "train_loss: 0.086144  [128064/200000]\n",
      "train_loss: 0.108494  [134464/200000]\n",
      "train_loss: 0.097275  [140864/200000]\n",
      "train_loss: 0.125597  [147264/200000]\n",
      "train_loss: 0.144725  [153664/200000]\n",
      "train_loss: 0.102948  [160064/200000]\n",
      "train_loss: 0.104927  [166464/200000]\n",
      "train_loss: 0.101157  [172864/200000]\n",
      "train_loss: 0.123746  [179264/200000]\n",
      "train_loss: 0.071272  [185664/200000]\n",
      "train_loss: 0.100614  [192064/200000]\n",
      "train_loss: 0.160660  [198464/200000]\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "train_loss 0.106733 val_loss 0.106537\n",
      "learning rate:  1e-05 , L:  200000 , sim:  0\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "train_loss 0.155848 val_loss 0.155999\n",
      "learning rate:  1e-05 , L:  200000 , sim:  0\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "train_loss 0.119379 val_loss 0.118602\n",
      "learning rate:  1e-05 , L:  200000 , sim:  0\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "train_loss 0.101829 val_loss 0.101373\n",
      "learning rate:  1e-05 , L:  200000 , sim:  0\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "train_loss 0.107939 val_loss 0.107322\n",
      "learning rate:  1e-05 , L:  200000 , sim:  0\n",
      "train_loss: 0.140123  [   64/200000]\n",
      "train_loss: 0.099421  [ 6464/200000]\n",
      "train_loss: 0.100279  [12864/200000]\n",
      "train_loss: 0.138952  [19264/200000]\n",
      "train_loss: 0.113620  [25664/200000]\n",
      "train_loss: 0.109051  [32064/200000]\n",
      "train_loss: 0.119752  [38464/200000]\n",
      "train_loss: 0.116851  [44864/200000]\n",
      "train_loss: 0.103494  [51264/200000]\n",
      "train_loss: 0.110980  [57664/200000]\n",
      "train_loss: 0.097754  [64064/200000]\n",
      "train_loss: 0.102463  [70464/200000]\n",
      "train_loss: 0.113061  [76864/200000]\n",
      "train_loss: 0.078844  [83264/200000]\n",
      "train_loss: 0.106891  [89664/200000]\n",
      "train_loss: 0.092625  [96064/200000]\n",
      "train_loss: 0.119394  [102464/200000]\n",
      "train_loss: 0.076137  [108864/200000]\n",
      "train_loss: 0.094407  [115264/200000]\n",
      "train_loss: 0.098311  [121664/200000]\n",
      "train_loss: 0.138292  [128064/200000]\n",
      "train_loss: 0.126223  [134464/200000]\n",
      "train_loss: 0.105721  [140864/200000]\n",
      "train_loss: 0.130044  [147264/200000]\n",
      "train_loss: 0.086251  [153664/200000]\n",
      "train_loss: 0.071562  [160064/200000]\n",
      "train_loss: 0.106163  [166464/200000]\n",
      "train_loss: 0.086214  [172864/200000]\n",
      "train_loss: 0.136215  [179264/200000]\n",
      "train_loss: 0.070518  [185664/200000]\n",
      "train_loss: 0.118202  [192064/200000]\n",
      "train_loss: 0.093044  [198464/200000]\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "train_loss 0.165492 val_loss 0.165810\n",
      "learning rate:  1e-05 , L:  200000 , sim:  0\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "train_loss 0.102468 val_loss 0.102142\n",
      "learning rate:  1e-05 , L:  200000 , sim:  0\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "train_loss 0.101976 val_loss 0.101841\n",
      "learning rate:  1e-05 , L:  200000 , sim:  0\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "train_loss 0.101876 val_loss 0.101291\n",
      "learning rate:  1e-05 , L:  200000 , sim:  0\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "train_loss 0.109744 val_loss 0.109829\n",
      "learning rate:  1e-05 , L:  200000 , sim:  0\n",
      "train_loss: 0.082320  [   64/200000]\n",
      "train_loss: 0.131330  [ 6464/200000]\n",
      "train_loss: 0.095946  [12864/200000]\n",
      "train_loss: 0.144902  [19264/200000]\n",
      "train_loss: 0.079202  [25664/200000]\n",
      "train_loss: 0.099083  [32064/200000]\n",
      "train_loss: 0.101048  [38464/200000]\n",
      "train_loss: 0.118791  [44864/200000]\n",
      "train_loss: 0.095161  [51264/200000]\n",
      "train_loss: 0.094786  [57664/200000]\n",
      "train_loss: 0.135428  [64064/200000]\n",
      "train_loss: 0.082749  [70464/200000]\n",
      "train_loss: 0.092257  [76864/200000]\n",
      "train_loss: 0.101498  [83264/200000]\n",
      "train_loss: 0.103419  [89664/200000]\n",
      "train_loss: 0.105037  [96064/200000]\n",
      "train_loss: 0.068118  [102464/200000]\n",
      "train_loss: 0.102786  [108864/200000]\n",
      "train_loss: 0.126374  [115264/200000]\n",
      "train_loss: 0.096688  [121664/200000]\n",
      "train_loss: 0.086847  [128064/200000]\n",
      "train_loss: 0.065219  [134464/200000]\n",
      "train_loss: 0.115527  [140864/200000]\n",
      "train_loss: 0.126011  [147264/200000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.077477  [153664/200000]\n",
      "train_loss: 0.081890  [160064/200000]\n",
      "train_loss: 0.126308  [166464/200000]\n",
      "train_loss: 0.110903  [172864/200000]\n",
      "train_loss: 0.099575  [179264/200000]\n",
      "train_loss: 0.131298  [185664/200000]\n",
      "train_loss: 0.089558  [192064/200000]\n",
      "train_loss: 0.094008  [198464/200000]\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "train_loss 0.113841 val_loss 0.113159\n",
      "learning rate:  1.0000000000000002e-06 , L:  200000 , sim:  0\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "train_loss 0.105831 val_loss 0.105295\n",
      "learning rate:  1.0000000000000002e-06 , L:  200000 , sim:  0\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "train_loss 0.105615 val_loss 0.105181\n",
      "learning rate:  1.0000000000000002e-06 , L:  200000 , sim:  0\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "train_loss 0.116747 val_loss 0.116503\n",
      "learning rate:  1.0000000000000002e-06 , L:  200000 , sim:  0\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "train_loss 0.113391 val_loss 0.113318\n",
      "learning rate:  1.0000000000000002e-06 , L:  200000 , sim:  0\n",
      "train_loss: 16.158878  [   64/300000]\n",
      "train_loss: 7.425385  [ 6464/300000]\n",
      "train_loss: 4.851234  [12864/300000]\n",
      "train_loss: 2.875977  [19264/300000]\n",
      "train_loss: 1.637164  [25664/300000]\n",
      "train_loss: 1.266094  [32064/300000]\n",
      "train_loss: 0.568243  [38464/300000]\n",
      "train_loss: 0.348139  [44864/300000]\n",
      "train_loss: 0.422761  [51264/300000]\n",
      "train_loss: 0.286517  [57664/300000]\n",
      "train_loss: 0.305452  [64064/300000]\n",
      "train_loss: 0.228163  [70464/300000]\n",
      "train_loss: 0.164781  [76864/300000]\n",
      "train_loss: 0.181471  [83264/300000]\n",
      "train_loss: 0.173825  [89664/300000]\n",
      "train_loss: 0.289221  [96064/300000]\n",
      "train_loss: 0.150007  [102464/300000]\n",
      "train_loss: 0.156806  [108864/300000]\n",
      "train_loss: 0.201697  [115264/300000]\n",
      "train_loss: 0.143039  [121664/300000]\n",
      "train_loss: 0.191562  [128064/300000]\n",
      "train_loss: 0.128318  [134464/300000]\n",
      "train_loss: 0.125537  [140864/300000]\n",
      "train_loss: 0.245716  [147264/300000]\n",
      "train_loss: 0.105438  [153664/300000]\n",
      "train_loss: 0.120968  [160064/300000]\n",
      "train_loss: 0.121364  [166464/300000]\n",
      "train_loss: 0.215627  [172864/300000]\n",
      "train_loss: 0.091349  [179264/300000]\n",
      "train_loss: 0.259697  [185664/300000]\n",
      "train_loss: 0.144455  [192064/300000]\n",
      "train_loss: 0.180057  [198464/300000]\n",
      "train_loss: 0.094608  [204864/300000]\n",
      "train_loss: 0.152655  [211264/300000]\n",
      "train_loss: 0.309959  [217664/300000]\n",
      "train_loss: 0.163629  [224064/300000]\n",
      "train_loss: 0.144791  [230464/300000]\n",
      "train_loss: 0.213997  [236864/300000]\n",
      "train_loss: 0.210040  [243264/300000]\n",
      "train_loss: 0.115381  [249664/300000]\n",
      "train_loss: 0.134109  [256064/300000]\n",
      "train_loss: 0.260234  [262464/300000]\n",
      "train_loss: 0.202092  [268864/300000]\n",
      "train_loss: 0.159940  [275264/300000]\n",
      "train_loss: 0.124364  [281664/300000]\n",
      "train_loss: 0.106641  [288064/300000]\n",
      "train_loss: 0.110472  [294464/300000]\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "train_loss 0.262033 val_loss 0.261019\n",
      "learning rate:  0.0001 , L:  300000 , sim:  0\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "train_loss 0.111247 val_loss 0.112245\n",
      "learning rate:  0.0001 , L:  300000 , sim:  0\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "train_loss 0.107013 val_loss 0.107906\n",
      "learning rate:  0.0001 , L:  300000 , sim:  0\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "train_loss 0.102666 val_loss 0.103772\n",
      "learning rate:  0.0001 , L:  300000 , sim:  0\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "train_loss 0.101641 val_loss 0.102641\n",
      "learning rate:  0.0001 , L:  300000 , sim:  0\n",
      "train_loss: 0.092074  [   64/300000]\n",
      "train_loss: 0.100345  [ 6464/300000]\n",
      "train_loss: 0.090099  [12864/300000]\n",
      "train_loss: 0.062042  [19264/300000]\n",
      "train_loss: 0.107712  [25664/300000]\n",
      "train_loss: 0.140653  [32064/300000]\n",
      "train_loss: 0.120868  [38464/300000]\n",
      "train_loss: 0.106073  [44864/300000]\n",
      "train_loss: 0.142275  [51264/300000]\n",
      "train_loss: 0.144103  [57664/300000]\n",
      "train_loss: 0.112488  [64064/300000]\n",
      "train_loss: 0.129017  [70464/300000]\n",
      "train_loss: 0.099838  [76864/300000]\n",
      "train_loss: 0.107436  [83264/300000]\n",
      "train_loss: 0.074246  [89664/300000]\n",
      "train_loss: 0.143314  [96064/300000]\n",
      "train_loss: 0.088192  [102464/300000]\n",
      "train_loss: 0.074294  [108864/300000]\n",
      "train_loss: 0.111663  [115264/300000]\n",
      "train_loss: 0.098866  [121664/300000]\n",
      "train_loss: 0.130337  [128064/300000]\n",
      "train_loss: 0.098899  [134464/300000]\n",
      "train_loss: 0.104511  [140864/300000]\n",
      "train_loss: 0.125753  [147264/300000]\n",
      "train_loss: 0.095555  [153664/300000]\n",
      "train_loss: 0.115058  [160064/300000]\n",
      "train_loss: 0.151596  [166464/300000]\n",
      "train_loss: 0.106541  [172864/300000]\n",
      "train_loss: 0.103222  [179264/300000]\n",
      "train_loss: 0.127890  [185664/300000]\n",
      "train_loss: 0.145079  [192064/300000]\n",
      "train_loss: 0.082290  [198464/300000]\n",
      "train_loss: 0.112597  [204864/300000]\n",
      "train_loss: 0.114374  [211264/300000]\n",
      "train_loss: 0.086378  [217664/300000]\n",
      "train_loss: 0.097280  [224064/300000]\n",
      "train_loss: 0.086541  [230464/300000]\n",
      "train_loss: 0.093525  [236864/300000]\n",
      "train_loss: 0.079911  [243264/300000]\n",
      "train_loss: 0.133714  [249664/300000]\n",
      "train_loss: 0.148899  [256064/300000]\n",
      "train_loss: 0.143692  [262464/300000]\n",
      "train_loss: 0.081085  [268864/300000]\n",
      "train_loss: 0.109791  [275264/300000]\n",
      "train_loss: 0.140342  [281664/300000]\n",
      "train_loss: 0.125669  [288064/300000]\n",
      "train_loss: 0.085436  [294464/300000]\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "train_loss 0.102992 val_loss 0.104149\n",
      "learning rate:  0.0001 , L:  300000 , sim:  0\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "train_loss 0.105081 val_loss 0.105991\n",
      "learning rate:  0.0001 , L:  300000 , sim:  0\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "train_loss 0.101065 val_loss 0.102207\n",
      "learning rate:  0.0001 , L:  300000 , sim:  0\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "train_loss 0.101278 val_loss 0.102340\n",
      "learning rate:  0.0001 , L:  300000 , sim:  0\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "train_loss 0.110569 val_loss 0.111359\n",
      "learning rate:  0.0001 , L:  300000 , sim:  0\n",
      "train_loss: 0.117335  [   64/300000]\n",
      "train_loss: 0.120887  [ 6464/300000]\n",
      "train_loss: 0.122767  [12864/300000]\n",
      "train_loss: 0.133955  [19264/300000]\n",
      "train_loss: 0.098906  [25664/300000]\n",
      "train_loss: 0.125450  [32064/300000]\n",
      "train_loss: 0.102914  [38464/300000]\n",
      "train_loss: 0.086613  [44864/300000]\n",
      "train_loss: 0.114750  [51264/300000]\n",
      "train_loss: 0.130948  [57664/300000]\n",
      "train_loss: 0.115765  [64064/300000]\n",
      "train_loss: 0.138333  [70464/300000]\n",
      "train_loss: 0.120063  [76864/300000]\n",
      "train_loss: 0.101331  [83264/300000]\n",
      "train_loss: 0.151674  [89664/300000]\n",
      "train_loss: 0.127013  [96064/300000]\n",
      "train_loss: 0.160955  [102464/300000]\n",
      "train_loss: 0.094497  [108864/300000]\n",
      "train_loss: 0.076397  [115264/300000]\n",
      "train_loss: 0.097257  [121664/300000]\n",
      "train_loss: 0.083624  [128064/300000]\n",
      "train_loss: 0.102660  [134464/300000]\n",
      "train_loss: 0.126846  [140864/300000]\n",
      "train_loss: 0.135184  [147264/300000]\n",
      "train_loss: 0.103223  [153664/300000]\n",
      "train_loss: 0.083272  [160064/300000]\n",
      "train_loss: 0.087112  [166464/300000]\n",
      "train_loss: 0.098978  [172864/300000]\n",
      "train_loss: 0.074534  [179264/300000]\n",
      "train_loss: 0.087616  [185664/300000]\n",
      "train_loss: 0.107877  [192064/300000]\n",
      "train_loss: 0.119922  [198464/300000]\n",
      "train_loss: 0.107979  [204864/300000]\n",
      "train_loss: 0.090629  [211264/300000]\n",
      "train_loss: 0.127591  [217664/300000]\n",
      "train_loss: 0.151574  [224064/300000]\n",
      "train_loss: 0.085262  [230464/300000]\n",
      "train_loss: 0.107395  [236864/300000]\n",
      "train_loss: 0.090540  [243264/300000]\n",
      "train_loss: 0.174604  [249664/300000]\n",
      "train_loss: 0.089795  [256064/300000]\n",
      "train_loss: 0.077615  [262464/300000]\n",
      "train_loss: 0.082580  [268864/300000]\n",
      "train_loss: 0.104792  [275264/300000]\n",
      "train_loss: 0.101451  [281664/300000]\n",
      "train_loss: 0.099660  [288064/300000]\n",
      "train_loss: 0.123018  [294464/300000]\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "train_loss 0.100159 val_loss 0.101330\n",
      "learning rate:  1e-05 , L:  300000 , sim:  0\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "train_loss 0.100547 val_loss 0.101878\n",
      "learning rate:  1e-05 , L:  300000 , sim:  0\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "train_loss 0.100306 val_loss 0.101492\n",
      "learning rate:  1e-05 , L:  300000 , sim:  0\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "train_loss 0.118183 val_loss 0.118691\n",
      "learning rate:  1e-05 , L:  300000 , sim:  0\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "train_loss 0.121027 val_loss 0.121630\n",
      "learning rate:  1e-05 , L:  300000 , sim:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.144217  [   64/300000]\n",
      "train_loss: 0.118400  [ 6464/300000]\n",
      "train_loss: 0.142932  [12864/300000]\n",
      "train_loss: 0.086186  [19264/300000]\n",
      "train_loss: 0.118927  [25664/300000]\n",
      "train_loss: 0.089540  [32064/300000]\n",
      "train_loss: 0.122554  [38464/300000]\n",
      "train_loss: 0.088770  [44864/300000]\n",
      "train_loss: 0.093079  [51264/300000]\n",
      "train_loss: 0.116921  [57664/300000]\n",
      "train_loss: 0.087637  [64064/300000]\n",
      "train_loss: 0.090423  [70464/300000]\n",
      "train_loss: 0.104021  [76864/300000]\n",
      "train_loss: 0.089750  [83264/300000]\n",
      "train_loss: 0.067905  [89664/300000]\n",
      "train_loss: 0.110376  [96064/300000]\n",
      "train_loss: 0.101388  [102464/300000]\n",
      "train_loss: 0.110055  [108864/300000]\n",
      "train_loss: 0.094283  [115264/300000]\n",
      "train_loss: 0.078386  [121664/300000]\n",
      "train_loss: 0.097645  [128064/300000]\n",
      "train_loss: 0.127707  [134464/300000]\n",
      "train_loss: 0.123362  [140864/300000]\n",
      "train_loss: 0.097724  [147264/300000]\n",
      "train_loss: 0.092757  [153664/300000]\n",
      "train_loss: 0.091203  [160064/300000]\n",
      "train_loss: 0.135090  [166464/300000]\n",
      "train_loss: 0.072782  [172864/300000]\n",
      "train_loss: 0.093988  [179264/300000]\n",
      "train_loss: 0.097065  [185664/300000]\n",
      "train_loss: 0.106623  [192064/300000]\n",
      "train_loss: 0.112337  [198464/300000]\n",
      "train_loss: 0.094298  [204864/300000]\n",
      "train_loss: 0.104234  [211264/300000]\n",
      "train_loss: 0.084205  [217664/300000]\n",
      "train_loss: 0.086088  [224064/300000]\n",
      "train_loss: 0.111091  [230464/300000]\n",
      "train_loss: 0.064860  [236864/300000]\n",
      "train_loss: 0.077355  [243264/300000]\n",
      "train_loss: 0.069437  [249664/300000]\n",
      "train_loss: 0.088800  [256064/300000]\n",
      "train_loss: 0.118441  [262464/300000]\n",
      "train_loss: 0.095951  [268864/300000]\n",
      "train_loss: 0.109109  [275264/300000]\n",
      "train_loss: 0.094485  [281664/300000]\n",
      "train_loss: 0.092332  [288064/300000]\n",
      "train_loss: 0.109801  [294464/300000]\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "train_loss 0.106707 val_loss 0.108179\n",
      "learning rate:  1e-05 , L:  300000 , sim:  0\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "train_loss 0.103353 val_loss 0.104428\n",
      "learning rate:  1e-05 , L:  300000 , sim:  0\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "train_loss 0.106440 val_loss 0.107443\n",
      "learning rate:  1e-05 , L:  300000 , sim:  0\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "train_loss 0.100176 val_loss 0.101524\n",
      "learning rate:  1e-05 , L:  300000 , sim:  0\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "train_loss 0.109788 val_loss 0.110622\n",
      "learning rate:  1e-05 , L:  300000 , sim:  0\n",
      "train_loss: 0.077566  [   64/300000]\n",
      "train_loss: 0.116987  [ 6464/300000]\n",
      "train_loss: 0.101924  [12864/300000]\n",
      "train_loss: 0.101748  [19264/300000]\n",
      "train_loss: 0.130861  [25664/300000]\n",
      "train_loss: 0.066574  [32064/300000]\n",
      "train_loss: 0.090115  [38464/300000]\n",
      "train_loss: 0.061518  [44864/300000]\n",
      "train_loss: 0.090800  [51264/300000]\n",
      "train_loss: 0.113130  [57664/300000]\n",
      "train_loss: 0.118853  [64064/300000]\n",
      "train_loss: 0.127360  [70464/300000]\n",
      "train_loss: 0.105960  [76864/300000]\n",
      "train_loss: 0.126695  [83264/300000]\n",
      "train_loss: 0.088476  [89664/300000]\n",
      "train_loss: 0.083133  [96064/300000]\n",
      "train_loss: 0.103723  [102464/300000]\n",
      "train_loss: 0.097523  [108864/300000]\n",
      "train_loss: 0.093457  [115264/300000]\n",
      "train_loss: 0.103457  [121664/300000]\n",
      "train_loss: 0.101884  [128064/300000]\n",
      "train_loss: 0.123830  [134464/300000]\n",
      "train_loss: 0.119561  [140864/300000]\n",
      "train_loss: 0.096530  [147264/300000]\n",
      "train_loss: 0.091345  [153664/300000]\n",
      "train_loss: 0.132323  [160064/300000]\n",
      "train_loss: 0.106275  [166464/300000]\n",
      "train_loss: 0.091165  [172864/300000]\n",
      "train_loss: 0.089105  [179264/300000]\n",
      "train_loss: 0.087664  [185664/300000]\n",
      "train_loss: 0.132221  [192064/300000]\n",
      "train_loss: 0.093491  [198464/300000]\n",
      "train_loss: 0.114051  [204864/300000]\n",
      "train_loss: 0.108940  [211264/300000]\n",
      "train_loss: 0.105375  [217664/300000]\n",
      "train_loss: 0.121453  [224064/300000]\n",
      "train_loss: 0.102270  [230464/300000]\n",
      "train_loss: 0.077786  [236864/300000]\n",
      "train_loss: 0.074965  [243264/300000]\n",
      "train_loss: 0.123962  [249664/300000]\n",
      "train_loss: 0.086586  [256064/300000]\n",
      "train_loss: 0.101350  [262464/300000]\n",
      "train_loss: 0.122339  [268864/300000]\n",
      "train_loss: 0.101900  [275264/300000]\n",
      "train_loss: 0.140094  [281664/300000]\n",
      "train_loss: 0.105786  [288064/300000]\n",
      "train_loss: 0.114338  [294464/300000]\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "train_loss 0.100043 val_loss 0.101248\n",
      "learning rate:  1.0000000000000002e-06 , L:  300000 , sim:  0\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "train_loss 0.100453 val_loss 0.101691\n",
      "learning rate:  1.0000000000000002e-06 , L:  300000 , sim:  0\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "train_loss 0.103797 val_loss 0.104946\n",
      "learning rate:  1.0000000000000002e-06 , L:  300000 , sim:  0\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "train_loss 0.119936 val_loss 0.120520\n",
      "learning rate:  1.0000000000000002e-06 , L:  300000 , sim:  0\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "train_loss 0.100354 val_loss 0.101650\n",
      "learning rate:  1.0000000000000002e-06 , L:  300000 , sim:  0\n",
      "train_loss: 15.982165  [   64/400000]\n",
      "train_loss: 8.898702  [ 6464/400000]\n",
      "train_loss: 4.445149  [12864/400000]\n",
      "train_loss: 3.084678  [19264/400000]\n",
      "train_loss: 2.171650  [25664/400000]\n",
      "train_loss: 1.208024  [32064/400000]\n",
      "train_loss: 0.847170  [38464/400000]\n",
      "train_loss: 0.228508  [44864/400000]\n",
      "train_loss: 0.229475  [51264/400000]\n",
      "train_loss: 0.192189  [57664/400000]\n",
      "train_loss: 0.239788  [64064/400000]\n",
      "train_loss: 0.200405  [70464/400000]\n",
      "train_loss: 0.233594  [76864/400000]\n",
      "train_loss: 0.189311  [83264/400000]\n",
      "train_loss: 0.221677  [89664/400000]\n",
      "train_loss: 0.436019  [96064/400000]\n",
      "train_loss: 0.147157  [102464/400000]\n",
      "train_loss: 0.165213  [108864/400000]\n",
      "train_loss: 0.120152  [115264/400000]\n",
      "train_loss: 0.218352  [121664/400000]\n",
      "train_loss: 0.185247  [128064/400000]\n",
      "train_loss: 0.159169  [134464/400000]\n",
      "train_loss: 0.202796  [140864/400000]\n",
      "train_loss: 0.170247  [147264/400000]\n",
      "train_loss: 0.172092  [153664/400000]\n",
      "train_loss: 0.216227  [160064/400000]\n",
      "train_loss: 0.162954  [166464/400000]\n",
      "train_loss: 0.160170  [172864/400000]\n",
      "train_loss: 0.175142  [179264/400000]\n",
      "train_loss: 0.129281  [185664/400000]\n",
      "train_loss: 0.336797  [192064/400000]\n",
      "train_loss: 0.177427  [198464/400000]\n",
      "train_loss: 0.185970  [204864/400000]\n",
      "train_loss: 0.133988  [211264/400000]\n",
      "train_loss: 0.215703  [217664/400000]\n",
      "train_loss: 0.275696  [224064/400000]\n",
      "train_loss: 0.148229  [230464/400000]\n",
      "train_loss: 0.172779  [236864/400000]\n",
      "train_loss: 0.192420  [243264/400000]\n",
      "train_loss: 0.251702  [249664/400000]\n",
      "train_loss: 0.142895  [256064/400000]\n",
      "train_loss: 0.298097  [262464/400000]\n",
      "train_loss: 0.257003  [268864/400000]\n",
      "train_loss: 0.123942  [275264/400000]\n",
      "train_loss: 0.144923  [281664/400000]\n",
      "train_loss: 0.206024  [288064/400000]\n",
      "train_loss: 0.118602  [294464/400000]\n",
      "train_loss: 0.186243  [300864/400000]\n",
      "train_loss: 0.149574  [307264/400000]\n",
      "train_loss: 0.132071  [313664/400000]\n",
      "train_loss: 0.150684  [320064/400000]\n",
      "train_loss: 0.210435  [326464/400000]\n",
      "train_loss: 0.326958  [332864/400000]\n",
      "train_loss: 0.202477  [339264/400000]\n",
      "train_loss: 0.103195  [345664/400000]\n",
      "train_loss: 0.103509  [352064/400000]\n",
      "train_loss: 0.151634  [358464/400000]\n",
      "train_loss: 0.139335  [364864/400000]\n",
      "train_loss: 0.190942  [371264/400000]\n",
      "train_loss: 0.145580  [377664/400000]\n",
      "train_loss: 0.147809  [384064/400000]\n",
      "train_loss: 0.132652  [390464/400000]\n",
      "train_loss: 0.164291  [396864/400000]\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "train_loss 0.136359 val_loss 0.137759\n",
      "learning rate:  0.0001 , L:  400000 , sim:  0\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "train_loss 0.120095 val_loss 0.120957\n",
      "learning rate:  0.0001 , L:  400000 , sim:  0\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "train_loss 0.101333 val_loss 0.102304\n",
      "learning rate:  0.0001 , L:  400000 , sim:  0\n"
     ]
    }
   ],
   "source": [
    "# Default : cuda\n",
    "torch.set_default_device('cuda')\n",
    "\n",
    "# Number of train and validation data\n",
    "Ls = [200000, 300000, 400000]\n",
    "L_val  = 50000\n",
    "\n",
    "# Define the batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Number of Epochs\n",
    "N_EPOCHS = 250\n",
    "    \n",
    "# Number of Simulations\n",
    "sims = 50\n",
    "\n",
    "for sim in range(0, sims):\n",
    "    tmp = \"syn_data/OU_sim_n3000.pt\"\n",
    "    tmp = torch.load(tmp)\n",
    "\n",
    "    # Data import\n",
    "    [X, Y] = tmp\n",
    "    X = X.to(device)\n",
    "    Y = Y.to(device)\n",
    "\n",
    "    torch.manual_seed(1000 + sim)\n",
    "    \n",
    "    for L in Ls:\n",
    "        L_train = L\n",
    "        indexes = torch.randperm(L_train + L_val)\n",
    "    \n",
    "        # Divide Data\n",
    "        X_train = X[indexes[0:L_train]]\n",
    "        Y_train = Y[indexes[0:L_train]]\n",
    "\n",
    "        X_val = X[indexes[L_train:(L_train + L_val)]]\n",
    "        Y_val = Y[indexes[L_train:(L_train + L_val)]]\n",
    "    \n",
    "        # Use torch.utils.data to create a DataLoader that will take care of creating batches\n",
    "        dataset = TensorDataset(X_train, Y_train)\n",
    "        dataloader = DataLoader(dataset, batch_size = BATCH_SIZE, shuffle=True, generator=torch.Generator(device=device))\n",
    "\n",
    "        # Get the dataset size for printing (it is equal to N_SAMPLES)\n",
    "        dataset_size = len(dataloader.dataset)\n",
    "\n",
    "        # Define the input and output dimensions\n",
    "        D_in, D_out = X_train.size()[1], Y_train.size()[1]\n",
    "\n",
    "\n",
    "        # Create an instance of the Net class with specified dimensions\n",
    "        net = Net(D_in = D_in, D_out = D_out)\n",
    "\n",
    "        # Model name\n",
    "        model_save_name = 'nets/SA2/net'+str(L)+\"_\"+str(sim)+'.pt'\n",
    "        path = F\"./{model_save_name}\"\n",
    "\n",
    "        # The nn package also contains definitions of popular loss functions; in this case we will use Mean Squared Error (MSE) as our loss function.\n",
    "        loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "        learning_rate = 1e-4\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "        train_error_plt = []\n",
    "        val_error_plt = []\n",
    "\n",
    "        torch.manual_seed(2000 + sim)\n",
    "\n",
    "        # Loop over epochs\n",
    "        for epoch in range(N_EPOCHS):\n",
    "            for id_batch, (x_batch, y_batch) in enumerate(dataloader):\n",
    "                y_batch_pred = net(x_batch)\n",
    "                loss = loss_fn(y_batch_pred, y_batch)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if epoch % 50 ==0 and id_batch % 100 == 0:\n",
    "                    loss, current = loss.item(), (id_batch + 1)* len(x_batch)\n",
    "                    print(f\"train_loss: {loss/BATCH_SIZE:>7f}  [{current:>5d}/{dataset_size:>5d}]\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                   net.eval()\n",
    "                   theta_pred_train = net(X_train)\n",
    "                   train_loss = loss_fn(theta_pred_train,Y_train) / L_train\n",
    "                   train_error_plt = np.append(train_error_plt, train_loss.to(\"cpu\"))\n",
    "\n",
    "                   theta_pred_val = net(X_val)\n",
    "                   val_loss = loss_fn(Y_val, theta_pred_val) / L_val\n",
    "                   val_error_plt = np.append(val_error_plt, val_loss.to(\"cpu\"))\n",
    "\n",
    "            if epoch % 10 ==0:\n",
    "                print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "                print(f\"train_loss {train_loss:>7f} val_loss {val_loss:>7f}\")\n",
    "                print(\"learning rate: \", learning_rate, \", L: \", L, \", sim: \", sim)\n",
    "\n",
    "            ## Choose Best Model\n",
    "            if val_error_plt[epoch] == np.min(val_error_plt):\n",
    "                 best=epoch\n",
    "                 torch.save(net.state_dict(), path)\n",
    "\n",
    "            if epoch % 100 ==99:\n",
    "                net.load_state_dict(torch.load(path))\n",
    "                learning_rate = max(learning_rate * 1e-1, 1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbe6a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad3c417",
   "metadata": {},
   "outputs": [],
   "source": [
    "#net.load_state_dict(torch.load(\"./OU_0.pt\"))\n",
    "net.load_state_dict(torch.load(path))\n",
    "\n",
    "torch.set_default_device('cpu')\n",
    "print(np.min(val_error_plt))\n",
    "print(np.argmin(val_error_plt))\n",
    "plt.plot(np.arange(N_EPOCHS), train_error_plt, color = \"r\")\n",
    "plt.plot(np.arange(N_EPOCHS), val_error_plt)\n",
    "plt.legend([\"train\", \"validation\"], loc =\"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd326ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to(torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e260a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypackages2",
   "language": "python",
   "name": "mypackages2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
